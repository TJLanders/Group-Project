{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in c:\\users\\thorj\\anaconda3\\lib\\site-packages (0.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"spacy\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import dask.bag as db\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "docs = db.read_text('arxiv-metadata-oai-snapshot.json').map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is very huge. Not sure if the whole set can be used. I start prototyping with a subset of the data so it's easyer to handel:\n",
    "# This procedure was recommended in the ArXiv dataset itself\n",
    "\n",
    "get_latest_version = lambda x: x['versions'][-1]['created']\n",
    "\n",
    "\n",
    "# get only necessary fields of the metadata file\n",
    "trim = lambda x: {'title': x['title'],\n",
    "                  'category':x['categories'].split(' '),}\n",
    "\n",
    "# filter for papers published on or after 2021-01-01\n",
    "columns = ['id','category','abstract']\n",
    "docs_df = (docs.filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2020)\n",
    "           .map(trim).\n",
    "           compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.to_csv(\"trimmed_arxiv_docs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./trimmed_arxiv_docs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "ml_identity = []\n",
    "for i in range(0, len(df['category'])):\n",
    "    if 'cs.LG' in df['category'][i]:\n",
    "        ml_identity.append(1)\n",
    "    else:\n",
    "        ml_identity.append(0)\n",
    "ml_identity = np.array(ml_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been 37088 papers in Machine learning since 2021\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in range(0, len(ml_identity)):\n",
    "    if ml_identity[i] == 1:\n",
    "        n += 1\n",
    "print('There have been', n, 'papers in Machine learning since 2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
